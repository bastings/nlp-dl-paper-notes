# (Incomplete) List of NLP papers that I read

Topics include Neural Machine Translation, Parsing, Syntax, and more. 
For now the papers are sorted by conference. 

## Arxiv

- Lee et al. (2017) [Recurrent Additive Networks](https://arxiv.org/abs/1705.07393)

## EMNLP'17

- Yang et al. (2017) [Towards Bidirectional Hierarchical Representations for Attention-Based Neural Machine Translation](http://aclweb.org/anthology/D17-1151)
- Gu et al. (2017) [Trainable Greedy Decoding for Neural Machine Translation](http://aclweb.org/anthology/D/D17/D17-1209.pdf)

## ACL'17

- Gehring et al. (2017) [A Convolutional Encoder Model for Neural Machine Translation](http://aclweb.org/anthology/P/P17/P17-1012.pdf)
- [Fully Character-Level Neural Machine Translation without Explicit Segmentation](https://arxiv.org/abs/1610.03017)

## ICLR'17

- [The Neural Noisy Channel](https://arxiv.org/abs/1611.02554)

## EACL'17

Kuncoro et al. [What Do Recurrent Neural Network Grammars Learn About Syntax?](http://aclweb.org/anthology/E/E17/E17-1117.pdf) [[bib]](http://aclweb.org/anthology/E/E17/E17-1117.bib)

## NIPS'16

- Gal & Ghahramani. [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks)

## CONLL'16

- Bowman et al. (2016) [Generating Sentences from a Continuous Space](http://www.aclweb.org/anthology/K16-1002)
- Kalchbrenner et al. (2016) [Neural Machine Translation in Linear Time](https://arxiv.org/abs/1610.10099)

## ACL'16

- Cross & Huang. [Incremental Parsing with Minimal Features Using Bi-Directional LSTM](http://aclweb.org/anthology/P/P16/P16-2006.pdf) [[bib]](http://aclweb.org/anthology/P/P16/P16-2006.bib)

## NAACL'16

Dyer et al. [Recurrent Neural Network Grammars](http://www.aclweb.org/anthology/N16-1024) [[bib]](http://aclweb.org/anthology/N/N16/N16-1024.bib)

## TACL'16

- Kiperwasser & Goldberg. [Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations](http://aclweb.org/anthology/Q/Q16/Q16-1023.pdf) [[bib]](http://aclweb.org/anthology/Q/Q16/Q16-1023.bib)

## Arxiv 

- Kalchbrenner et al. (2016) [Neural Machine Translation in Linear Time](https://arxiv.org/abs/1610.10099)

## ACL'15

- Dyer et al. [Transition-Based Dependency Parsing with Stack Long Short-Term Memory](http://aclweb.org/anthology/P/P15/P15-1033.pdf) [[bib]](http://aclweb.org/anthology/P/P15/P15-1033.bib

## Misc

- Rush et al. (2015). [Neural Attention Model for Sentence Summarization](https://arxiv.org/abs/1509.00685)

# Summaries

- Parikh et al. (2016). [A Decomposable Attention Model for Natural Language Inference](https://github.com/bastings/nlp-dl-paper-notes/blob/master/notes/a-decomposable-attention-model-for-nli.md)

