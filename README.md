# (Incomplete) List of NLP papers that I read

Topics include Neural Machine Translation, Parsing, Syntax, and more. 
For now the papers are sorted by conference. 

## Arxiv 2017

- Lee et al. [Recurrent Additive Networks](https://arxiv.org/abs/1705.07393)

## EMNLP 2017 Copenhagen

- Yang et al. [Towards Bidirectional Hierarchical Representations for Attention-Based Neural Machine Translation](http://aclweb.org/anthology/D17-1151)
- Gu et al. (2017) [Trainable Greedy Decoding for Neural Machine Translation](http://aclweb.org/anthology/D/D17/D17-1209.pdf)

## ACL 2017 Vancouver

- Gehring et al. [A Convolutional Encoder Model for Neural Machine Translation](http://aclweb.org/anthology/P/P17/P17-1012.pdf)
- [Fully Character-Level Neural Machine Translation without Explicit Segmentation](https://arxiv.org/abs/1610.03017)

## ICLR 2017 Toulon

- Yu et al. [The Neural Noisy Channel](https://arxiv.org/abs/1611.02554)
- Dozat & Manning. [Deep Biaffine Attention for Neural Dependency Parsing](https://openreview.net/forum?id=Hk95PK9le&noteId=Hk95PK9le) 

## EACL 2017 

Kuncoro et al. [What Do Recurrent Neural Network Grammars Learn About Syntax?](http://aclweb.org/anthology/E/E17/E17-1117.pdf) [[bib]](http://aclweb.org/anthology/E/E17/E17-1117.bib)

## NIPS 2016

- Gal & Ghahramani. [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks)

## CONLL 2016

- Bowman et al. (2016) [Generating Sentences from a Continuous Space](http://www.aclweb.org/anthology/K16-1002)
- Kalchbrenner et al. (2016) [Neural Machine Translation in Linear Time](https://arxiv.org/abs/1610.10099)

## ACL'16

- Cross & Huang. [Incremental Parsing with Minimal Features Using Bi-Directional LSTM](http://aclweb.org/anthology/P/P16/P16-2006.pdf) [[bib]](http://aclweb.org/anthology/P/P16/P16-2006.bib)

## EMNLP 2016

- Parikh et al. [A Decomposable Attention Model for Natural Language Inference](https://aclweb.org/anthology/D16-1244)
[[summary]](https://github.com/bastings/nlp-dl-paper-notes/blob/master/notes/a-decomposable-attention-model-for-nli.md)

## NAACL 2016

Dyer et al. [Recurrent Neural Network Grammars](http://www.aclweb.org/anthology/N16-1024) [[bib]](http://aclweb.org/anthology/N/N16/N16-1024.bib)

## TACL 2016

- Kiperwasser & Goldberg. [Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations](http://aclweb.org/anthology/Q/Q16/Q16-1023.pdf) [[bib]](http://aclweb.org/anthology/Q/Q16/Q16-1023.bib)

## Arxiv 2016

- Kalchbrenner et al. (2016) [Neural Machine Translation in Linear Time](https://arxiv.org/abs/1610.10099)

## ACL 2015

- Dyer et al. [Transition-Based Dependency Parsing with Stack Long Short-Term Memory](http://aclweb.org/anthology/P/P15/P15-1033.pdf) [[bib]](http://aclweb.org/anthology/P/P15/P15-1033.bib

## Misc

- Rush et al. (2015). [Neural Attention Model for Sentence Summarization](https://arxiv.org/abs/1509.00685)
